{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LSTM, Activation\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC's represent distinct units of sound or phonemes as the shape of the vocal tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_len = 11\n",
    "labels = os.listdir(path)\n",
    "for label in labels:\n",
    "    mfcc_vectors = []\n",
    "    wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "    for wavfile in wavfiles:\n",
    "        wav, _ = librosa.load(wavfile, mono=True, sr = None)\n",
    "        wav = np.asfortranarray(wav[::3])\n",
    "        mfcc = librosa.feature.mfcc(wav, sr=22050, n_mfcc=40)\n",
    "        if (max_len > mfcc.shape[1]):\n",
    "            pad_width = max_len - mfcc.shape[1]\n",
    "            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_len]\n",
    "        mfcc_vectors.append(mfcc)\n",
    "    np.save(label + '.npy', mfcc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'bird', 'happy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices = np.arange(0, len(labels))\n",
    "label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(labels[0] + '.npy')\n",
    "y = np.zeros(X.shape[0])\n",
    "\n",
    "for i, label in enumerate(labels[1:]):\n",
    "    x = np.load(label + '.npy')\n",
    "    X = np.vstack((X, x))\n",
    "    y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "assert X.shape[0] == len(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5206, 40, 11)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5206,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3644"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3644, 40, 11, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 40, 11, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 40, 11, 1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAD6CAYAAACxpykjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANAklEQVR4nO2dbYxcVRnHf89sd7t0aYBCqaUlYggRkWhNCprwBUFMJSbVRI01MZqQoIkkGo2h6gcl0YQPKl80Goy1mCiFoERi8KVWDCExSIuI5b1WkKVLC5TCAna7nXn8MHdxuvc5O/dlZvrM7vNLNrNz9t65Z/e/557nnvM/zxFVJTi5NE52BYIQwQUhggNCBAeECA4IERxQSwQR2SQiT4jIPhHZ2qtKLTWk6nOCiIwATwJXAZPAA8AWVX00dc6YLNdxJipdD0Aa9v/MzFtOyZUtm5g1jx0fOZ4ra6r9uf+dHc3X4Q372NFXm7my6TemXlTV1eYJHSzrdsACXArsU9X9ACKyA9gMJEUYZ4L3ypWVL9g4daVZvv8LF+fK1lzyvHns+ae9mCubnl1uHrt3am2ubNkeuw7r//RKrmzn7hueMQ+eR53b0Trg2Y73k1lZUJI6LUGMsty9TUSuBa4FGGdFjcstXuq0hEng3I7364ED8w9S1ZtVdaOqbhzFbvZLnTod8zLaHfOVwHO0O+ZPqeojqXPe+a4xvf23XfupJC2z8dlcOGoLPiL5/7sZtTvxg82ZXNmzx+3WfNHo0VzZWesP7FHVjQvVE2rcjlT1uIhcB/wBGAG2LSRAkKZOn4Cq3g3c3aO6LFniidkBIYIDQgQH1OoT+okVCb3Uyg9PAMzqSK7s8WP28MiRZj66GZGWeex5o/mn63OWTZvHvmB/RCGiJTggRHBAiOCAEMEBbjtmi9Mb+aGBFC81Ty187Kjk5wIAGkaH/UbL/pM91zzNKJ0qdP1oCQ4IERwQIjggRHBAiOAAt9FRIz9TysqGHcWsNCZqRsfsyMQaDmkmJotGyUdHEw17fOL5ZnV3e7QEB4QIDggRHFCrTxCRp4FpoAkcLzKpHeTpRcf8flXND7zXZETyHV2q2T5v9NcHSwxbrGjkXRVgd+IzzfzcBcA5y/IOvKLE7cgBdUVQ4I8isidz2gUVqHs7ukxVD4jI2cBOEXlcVe/tPKDTBrl2nd2Ulzq1WoKqHsheDwF30nZqzz/mTRvkGavi7mdR+a8iIhMisnLue+CDwN5eVWwpUed2tAa4U0TmPueXqvr7ntQKaGo+MjnQGjOPPdOIblIRj+XMaCUWiSxv5D2q4wlnxgut6mbnOl7U/cC7K185eJO4STsgRHBAiOAAt/MJ1pDBhORXXgIcMTrsEWM+AmDE+IzUsTNGJz6W6JjHE46NIkRLcECI4IAQwQEhggNCBAe4jY4sUhHIUSOKGU1EMbPGEEXSbZH4DItUhFWEaAkOCBEcECI4IERwgNuO2bJBvq52da0Oe0WiEx8x/u2OGnMX7fJ8h5+qQx2iJTggRHBAiOCAEMEBXXsZEdkGfBg4pKoXZ2WrgNuA84CngU+o6sv9q2ab1BOz9bR6NDF5P9Mq/nRtXc964ob0U3cRirSE7cCmeWVbgV2qegGwK3sfVKSrCJmj7vC84s3ALdn3twAf6XG9lhRV+4Q1qjoFkL2enTpQRK4Vkd0isvvlwzVSoSxi+t4xhw2yO1X/KgdFZC1A9nqod1VaelR9Br8L+AxwY/b6m57VqAJWZLJ6xB7ftyKhViIt6ZFWvtxO3pmOmorQ9UwRuRX4K/B2EZkUkWto//GvEpGnaCcsv7FyDYLuLUFVtyR+VD3zeHAC0VM6IERwgNv5hLqUeSJpiD3kMNEwylv20EnYIIecEMEBIYIDQgQHhAgOGKroqMyEynTLXlByzIhixhPRUdMYzkhOFhnOjKJES3BAiOCAEMEBIYIDhqpjTjkaLLeFtaIT7OGFmVSu7BJ1W9GwA4EiREtwQIjggBDBASGCA4rMMW8TkUMisrej7Fsi8pyIPJR9Xd3fai5uikRH24EfAD+fV36Tqn635zVagJWJ3BavtPI7iB9NLOawkk69nvCijhlTQynfaiux0KQIVW2QQQ+p0ydcJyIPZ7erM3pWoyVIVRF+BJwPbKC9e8/3UgeGF7U7lURQ1YOq2lTVFvATjFScHceGF7ULlYYtRGTtnCsb+CgDSsWZGsu3hjNS23ZZQxxlFnhYq0oBZmvc2Yus1LkVuBw4S0QmgW8Cl4vIBtppmp8GPle5BkFlG+RP+1CXJUvcpB0QIjggRHDAUE3qWGk6wR5KGE9FMUaEZQ1lADSN3UxSRNKpISdEcECI4IAQwQFD1TGnhgwsUos27P07U2syjSMTVeh3bougz4QIDggRHBAiOCBEcMBQRUcprKhp2nBggD3EkYq6rE1YUwtVTkvsml6EaAkOCBEcECI4oIgN8lwRuUdEHhORR0Tki1n5KhHZKSJPZa/hPapIkY75OPAVVX0w2/B0j4jsBD5LOyPkjSKylXZGyOv7V1W7owQ7p/WxxP/XsRLJocY034kvTwyHzFSfTihkg5xS1Qez76eBx4B1REbInlGqTxCR84D3APdTIiNksDCFRRCRU4FfAV9S1VdLnBc2yC4UEkFERmkL8AtV/XVWXCgjZNggu1PEgSe0zV6Pqer3O3408IyQqadVa5I9NZ9gfUZqkt56uk7tvVlnc4siZ14GfBr4p4g8lJV9nfYf//YsO+R/gI9XrsUSp4gN8j5IThtFRsgeEDdpB4QIDggRHDBU8wll9tNcVSLXROrpZdrYdSQVBaV2TS9CtAQHhAgOCBEcECI4YKg65mYidYHVYafG90eNj0j9J1r7d5ZZQVqUaAkOCBEcECI4IERwQIjggKGKjlKrN61JmZSr4lgJV0TK3WGRityKEC3BASGCA0IEB9SxQUZGyB5RxwYJA84ImUz4ZHTCZVZ69oK+Jp3K3HVzTrtpEZmzQQY9oo4NEiIjZE+oY4MslBEybJDdqWyDLJoRMmyQ3SkSHZk2yDkfasbAMkIuRurYILd4yQhpRUIpz2iZtmh9QioKqpN0qo4N8u7KVw1OIG7SDggRHBAiOGCo5hNSWJ1lakFJmR3Ey3S2KYtmEaIlOCBEcECI4IAQwQEhggOGKjpKDRlYCzRSSacOt8ZzZY1EZLNS8qk6U1FQuC2GnBDBASGCA0IEBwxVxzyaWGdpLdBI7RS+XPPDFil7pRUItNQeyqjj7oiW4IAQwQEhggOKTPSPi8jfROQfmQ3yhqz8bSJyf5YN8jYRGet/dRcnRTrmGeAKVX0ts77cJyK/A75M2wa5Q0R+DFxD24vUN8pkg0zNG1gGgNTKy0aNp+AyFMkGqar6WvZ2NPtS4Argjqw8skHWoKj5aySzuxwCdgL/Ao6o6lwcOEn4UytTSITMabcBWE/bafcO6zDr3LBBdqdUdKSqR4C/AO8DTheRuT5lPXAgcU7YILtQJDpaLSKnZ9+fAnyAdpbge4CPZYcNJBvkYqVIdLQWuEVERmiLdruq/lZEHgV2iMi3gb8zgD2ak7ktkmmj8lirOsus0iyz/2dRitggH6a9JmF++X4STuygHHGTdkCI4IAQwQFu5xOsDjA1Zm8NRVhDGWDbI1MdvmWDTM1TxKbYQ06I4IAQwQEhggNCBAe4jY6sSCg1ZGANRaSGEazyVHRUJl9Fma3D5hMtwQEhggNCBAeECA5w2zFbpIYtzA67xChCqsNvGR12ypmR+owiREtwQIjggBDBAXVskNtF5N8d2SA39L+6i5M6NkiAr6rqHQucGxSgyES/ApYN0jWpIQcr4mkk3BbWLuQpZ4b1uUWpZINU1blskN/JskHeJCLLK9diiVPJBikiFwNfAy4ELgFWAddb54YNsjtVbZCbVHUqc2zPAD8jskFWpqoN8vGOXcmFti0+skFWpI4N8s8ispp2ksKHgM/3sZ7AAnZFozi5mtLoP3uxerPOFl91bJBXVL5qcAJxk3ZAiOCAEMEBIYID3E7qlJmoKTOhYrktxkosMkmR2rG8CNESHBAiOCBEcECI4AC3HbM17JCaIyizetPMoZ2yTJp1qL+CdD7REhwQIjggRHBAiOCAEMEBbqMji1QEUmYxh7XUNbXriBU1pY6NnUSGnBDBASGCA0IEB4gm3AN9uZjIC8Az2duzgBcHdvHB0fl7vVVVV3c7YaAinHBhkd2quvGkXLyPVPm94nbkgBDBASdThJtP4rX7Senf66T1CcH/iduRAwYugohsEpEnRGSfiGwd9PV7iYhsE5FDIrK3o2yViOzMtjTYKSJndPucgYqQObt/CHwIuAjYIiIXDbIOPWY7sGle2VZgl6peAOzK3i/IoFvCpcA+Vd2vqseAHcDmAdehZ6jqvcDhecWbaW9lAAW3NBi0COuAZzveL8aU/2tUdQogez272wmDFsGyKiz58GzQIkwC53a8T6b8H2IOdiwlW0t7xeuCDFqEB4ALsk2RxoBPAncNuA795i7aWxlA0S0NVHWgX8DVwJO0t4T5xqCv3+Pf5VZgCpil3cqvAc6kHRU9lb2u6vY58cTsgHhidkCI4IAQwQEhggNCBAeECA4IERwQIjjgf3RYPCoKBQL0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[105, :, :, 0])\n",
    "print(y_train[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 38, 9, 12)         120       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4104)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 12315     \n",
      "=================================================================\n",
      "Total params: 12,435\n",
      "Trainable params: 12,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(12, kernel_size=3, input_shape=(40,11,1), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3644 samples, validate on 1562 samples\n",
      "Epoch 1/2\n",
      "3644/3644 [==============================] - 6s 2ms/step - loss: 1.1908 - accuracy: 0.8002 - val_loss: 0.6513 - val_accuracy: 0.8611\n",
      "Epoch 2/2\n",
      "3644/3644 [==============================] - 5s 1ms/step - loss: 0.3843 - accuracy: 0.9059 - val_loss: 0.5636 - val_accuracy: 0.8784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f86a5ac6fd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 40, 11, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 40, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 20, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 20, 5, 16)         9232      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 4803      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 51,603\n",
      "Trainable params: 51,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64,activation='relu',kernel_size=3,input_shape=(40,11,1),padding='same')) #input_shape does not include batch_size\n",
    "model.add(Conv2D(64,activation='relu',kernel_size=3,padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(16,activation='relu',kernel_size=3,padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3644 samples, validate on 1562 samples\n",
      "Epoch 1/6\n",
      "3644/3644 [==============================] - 10s 3ms/step - loss: 0.0911 - accuracy: 0.9701 - val_loss: 0.1549 - val_accuracy: 0.9513\n",
      "Epoch 2/6\n",
      "3644/3644 [==============================] - 11s 3ms/step - loss: 0.0713 - accuracy: 0.9759 - val_loss: 0.2004 - val_accuracy: 0.9373\n",
      "Epoch 3/6\n",
      "3644/3644 [==============================] - 11s 3ms/step - loss: 0.0945 - accuracy: 0.9668 - val_loss: 0.1207 - val_accuracy: 0.9622\n",
      "Epoch 4/6\n",
      "3644/3644 [==============================] - 10s 3ms/step - loss: 0.0618 - accuracy: 0.9761 - val_loss: 0.1576 - val_accuracy: 0.9590\n",
      "Epoch 5/6\n",
      "3644/3644 [==============================] - 11s 3ms/step - loss: 0.0361 - accuracy: 0.9882 - val_loss: 0.1177 - val_accuracy: 0.9635\n",
      "Epoch 6/6\n",
      "3644/3644 [==============================] - 11s 3ms/step - loss: 0.0415 - accuracy: 0.9852 - val_loss: 0.1530 - val_accuracy: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f86a96e81d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 6, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3644, 40, 11)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 40, 11)\n",
    "X_test = X_test.reshape(X_test.shape[0], 40, 11)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(40, 11)))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3644 samples, validate on 1562 samples\n",
      "Epoch 1/10\n",
      "3644/3644 [==============================] - 50s 14ms/step - loss: 0.9717 - accuracy: 0.5058 - val_loss: 0.8724 - val_accuracy: 0.5992\n",
      "Epoch 2/10\n",
      "3644/3644 [==============================] - 47s 13ms/step - loss: 0.7640 - accuracy: 0.6641 - val_loss: 0.7287 - val_accuracy: 0.6889\n",
      "Epoch 3/10\n",
      "3644/3644 [==============================] - 46s 13ms/step - loss: 0.5989 - accuracy: 0.7610 - val_loss: 0.6722 - val_accuracy: 0.7273\n",
      "Epoch 4/10\n",
      "3644/3644 [==============================] - 45s 12ms/step - loss: 0.4864 - accuracy: 0.8071 - val_loss: 0.6330 - val_accuracy: 0.7497\n",
      "Epoch 5/10\n",
      "3644/3644 [==============================] - 47s 13ms/step - loss: 0.3990 - accuracy: 0.8513 - val_loss: 0.6824 - val_accuracy: 0.7420\n",
      "Epoch 6/10\n",
      "3644/3644 [==============================] - 47s 13ms/step - loss: 0.3067 - accuracy: 0.8878 - val_loss: 0.6171 - val_accuracy: 0.7849\n",
      "Epoch 7/10\n",
      "3644/3644 [==============================] - 44s 12ms/step - loss: 0.2399 - accuracy: 0.9061 - val_loss: 0.7060 - val_accuracy: 0.7638\n",
      "Epoch 8/10\n",
      "3644/3644 [==============================] - 47s 13ms/step - loss: 0.1875 - accuracy: 0.9308 - val_loss: 0.8087 - val_accuracy: 0.7791\n",
      "Epoch 9/10\n",
      "3644/3644 [==============================] - 48s 13ms/step - loss: 0.1421 - accuracy: 0.9468 - val_loss: 0.7305 - val_accuracy: 0.7830\n",
      "Epoch 10/10\n",
      "3644/3644 [==============================] - 51s 14ms/step - loss: 0.0996 - accuracy: 0.9630 - val_loss: 0.8299 - val_accuracy: 0.7900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f86a22e4fd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_train[0].reshape(1,40,11))\n",
    "pred.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SpeechRecognition api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech_recognition.AudioData"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()\n",
    "bird = sr.AudioFile('bird.wav')\n",
    "with bird as source:\n",
    "    audio = r.record(source)\n",
    "type(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bird'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.recognize_google(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
